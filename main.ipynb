{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prerequisits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I use pandas to collect the data from the URL and save it locally for spark to read it\n",
    "# i have searched everywhere to find a way to read in the dataset from the URL in parrelel but could not find it\n",
    "import pandas as pd\n",
    "\n",
    "url = 'https://raw.githubusercontent.com/jpatokal/openflights/master/data/routes.dat'\n",
    "\n",
    "# get the data from the URL and cast it to the variable pandas_df as a dataframe\n",
    "pandas_df = pd.read_csv(url, names=[\"Airline\", \"Airline ID\", \"Source airport\", \n",
    "                                    \"Source airport ID\", \"Destination airport\", \"Destination airport ID\", \"Codeshare\", \"Stops\", \"Equipment\"])\n",
    "# save the file \n",
    "pandas_df.to_csv('routes.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+----------+--------------+-----------------+-------------------+----------------------+---------+-----+---------+\n",
      "|_c0|Airline|Airline ID|Source airport|Source airport ID|Destination airport|Destination airport ID|Codeshare|Stops|Equipment|\n",
      "+---+-------+----------+--------------+-----------------+-------------------+----------------------+---------+-----+---------+\n",
      "|  0|     2B|       410|           AER|             2965|                KZN|                  2990|     null|    0|      CR2|\n",
      "|  1|     2B|       410|           ASF|             2966|                KZN|                  2990|     null|    0|      CR2|\n",
      "|  2|     2B|       410|           ASF|             2966|                MRV|                  2962|     null|    0|      CR2|\n",
      "|  3|     2B|       410|           CEK|             2968|                KZN|                  2990|     null|    0|      CR2|\n",
      "|  4|     2B|       410|           CEK|             2968|                OVB|                  4078|     null|    0|      CR2|\n",
      "|  5|     2B|       410|           DME|             4029|                KZN|                  2990|     null|    0|      CR2|\n",
      "|  6|     2B|       410|           DME|             4029|                NBC|                  6969|     null|    0|      CR2|\n",
      "|  7|     2B|       410|           DME|             4029|                TGK|                    \\N|     null|    0|      CR2|\n",
      "|  8|     2B|       410|           DME|             4029|                UUA|                  6160|     null|    0|      CR2|\n",
      "|  9|     2B|       410|           EGO|             6156|                KGD|                  2952|     null|    0|      CR2|\n",
      "| 10|     2B|       410|           EGO|             6156|                KZN|                  2990|     null|    0|      CR2|\n",
      "| 11|     2B|       410|           GYD|             2922|                NBC|                  6969|     null|    0|      CR2|\n",
      "| 12|     2B|       410|           KGD|             2952|                EGO|                  6156|     null|    0|      CR2|\n",
      "| 13|     2B|       410|           KZN|             2990|                AER|                  2965|     null|    0|      CR2|\n",
      "| 14|     2B|       410|           KZN|             2990|                ASF|                  2966|     null|    0|      CR2|\n",
      "| 15|     2B|       410|           KZN|             2990|                CEK|                  2968|     null|    0|      CR2|\n",
      "| 16|     2B|       410|           KZN|             2990|                DME|                  4029|     null|    0|      CR2|\n",
      "| 17|     2B|       410|           KZN|             2990|                EGO|                  6156|     null|    0|      CR2|\n",
      "| 18|     2B|       410|           KZN|             2990|                LED|                  2948|     null|    0|      CR2|\n",
      "| 19|     2B|       410|           KZN|             2990|                SVX|                  2975|     null|    0|      CR2|\n",
      "+---+-------+----------+--------------+-----------------+-------------------+----------------------+---------+-----+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# The entry point into all functionality in Spark is the SparkSession class\n",
    "# To create a basic SparkSession, just use SparkSession.builder\n",
    "spark = SparkSession.builder.appName(\"Airline Routes\").getOrCreate()\n",
    "\n",
    "# this will output a link named Spark UI to see more about your sessions\n",
    "spark\n",
    "\n",
    "df = spark.read.csv('routes.csv', inferSchema=True, nullValue='NA', header=True)\n",
    "\n",
    "adf = df.groupBy('Source airport').count()\n",
    "\n",
    "from pyspark.sql.functions import desc, asc\n",
    "\n",
    "adf = adf.sort(desc(\"count\"))\n",
    "save_df = adf.limit(10)\n",
    "save_df = save_df.toPandas()\n",
    "save_df.to_csv('top_10_airports.csv')\n",
    "\n",
    "df.show()\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# By reading the theory. The stream of data is regarded as a table to which data is continiously appended\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import types\n",
    "from pyspark.sql.types import StructType\n",
    "\n",
    "# Read all the csv files written atomically in a directory\n",
    "spark = SparkSession.builder.appName(\"Structured Streaming\").getOrCreate()\n",
    "\n",
    "# refrence example\n",
    "# userSchema = StructType().add(\"name\", \"string\").add(\"age\", \"integer\")\n",
    "\n",
    "userSchema = StructType().add(\"Airline\", \"string\").add(\"Airline ID\", \"integer\").add(\"Source airport\", \"string\")\\\n",
    ".add(\"Source airport ID\", \"integer\").add(\"Destination airport\", \"string\").add(\"Destination airport ID\", \"integer\")\\\n",
    ".add(\"Codeshare\", \"string\").add(\"Stops\", \"integer\").add(\"Equipment\", \"string\")\n",
    "\n",
    "csv_sdf = spark.readStream.csv('routes.csv', schema = userSchema)\n",
    "\n",
    "csv_sdf.isStreaming\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv_sdf.writeStream.format(\"memory\").queryName(\"whole\").start()\n",
    "# spark.sql(\"select * from whole\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_sdf.createOrReplaceTempView(\"planes\")\n",
    "planes = spark.sql(\"select * from planes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = planes.writeStream.format(\"console\").start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "ename": "StreamingQueryException",
     "evalue": "[STREAM_FAILED] Query [id = 64a0a879-c945-4a19-a32f-c178fcf61699, runId = 684c4aab-fc94-445a-af62-1e78538fcae8] terminated with exception: org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mStreamingQueryException\u001b[0m                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[101], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m query\u001b[39m.\u001b[39;49mawaitTermination()\n",
      "File \u001b[1;32mc:\\Users\\ozcan\\.pyenv\\pyenv-win\\versions\\3.11.2\\Lib\\site-packages\\pyspark\\sql\\streaming\\query.py:201\u001b[0m, in \u001b[0;36mStreamingQuery.awaitTermination\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    199\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jsq\u001b[39m.\u001b[39mawaitTermination(\u001b[39mint\u001b[39m(timeout \u001b[39m*\u001b[39m \u001b[39m1000\u001b[39m))\n\u001b[0;32m    200\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 201\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jsq\u001b[39m.\u001b[39;49mawaitTermination()\n",
      "File \u001b[1;32mc:\\Users\\ozcan\\.pyenv\\pyenv-win\\versions\\3.11.2\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[0;32m   1323\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[0;32m   1325\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(temp_arg, \u001b[39m\"\u001b[39m\u001b[39m_detach\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\ozcan\\.pyenv\\pyenv-win\\versions\\3.11.2\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:175\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    171\u001b[0m converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n\u001b[0;32m    172\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(converted, UnknownException):\n\u001b[0;32m    173\u001b[0m     \u001b[39m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[0;32m    174\u001b[0m     \u001b[39m# JVM exception message.\u001b[39;00m\n\u001b[1;32m--> 175\u001b[0m     \u001b[39mraise\u001b[39;00m converted \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m    176\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    177\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "\u001b[1;31mStreamingQueryException\u001b[0m: [STREAM_FAILED] Query [id = 64a0a879-c945-4a19-a32f-c178fcf61699, runId = 684c4aab-fc94-445a-af62-1e78538fcae8] terminated with exception: org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z"
     ]
    }
   ],
   "source": [
    "query.awaitTermination()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Window Operations on Event Time\n",
    "\n",
    "Aggregations over a sliding event-time window are straightforward with Structured Streaming and are very similar to grouped aggregations. In a grouped aggregation, aggregate values (e.g. counts) are maintained for each unique value in the user-specified grouping column. In case of window-based aggregations, aggregate values are maintained for each window the event-time of a row falls into. Let’s understand this with an illustration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import window\n",
    "path = \"C:/Users/ozcan/Desktop/Schiphol\"\n",
    "agg_csv = csv_sdf.groupBy(\"Source airport\").count()\n",
    "# agg_csv = csv_sdf.groupBy(\"Source airport\",window(\"Airline\", windowDuration=\"10 minutes\", slideDuration=\"5 minutes\")).count()\n",
    "agg_csv.isStreaming"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
